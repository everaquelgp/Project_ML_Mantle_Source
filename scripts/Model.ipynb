{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and testing my model with data that was labeled based only on 143Nd/144Nd versus 87Sr/86Sr values. \n",
    "\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "device = \"cuda\" #to use my GPU\n",
    "\n",
    "final_data=pd.read_csv('../DataFP/final_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       labelSrNd   latitude   longitude  rb87_sr86  sr87_sr86  nd143_nd144   \n",
      "0              3  33.686551  130.304669     0.6262   0.705200     0.512554  \\\n",
      "1              3  33.687129  130.304404     0.5366   0.705170     0.512661   \n",
      "2              3  33.688168  130.301608     0.5165   0.705010     0.512694   \n",
      "3              3  36.988056  138.505000        NaN   0.703871     0.512919   \n",
      "4              3  43.076423  141.635239        NaN   0.705267     0.512626   \n",
      "...          ...        ...         ...        ...        ...          ...   \n",
      "13853          1 -63.000000  -64.000000        NaN   0.702640     0.512980   \n",
      "13854          1 -63.000000  -64.000000        NaN   0.702830     0.512960   \n",
      "13855          1 -60.333500  -29.709667        NaN   0.703100     0.513016   \n",
      "13856          1 -60.200000  -29.869833        NaN   0.702940     0.513063   \n",
      "13857          1 -60.054167  -29.970167        NaN   0.703102     0.513001   \n",
      "\n",
      "       sm147_nd144  pb206_pb204  pb207_pb204  pb208_pb204  ...  ta_ppm   \n",
      "0           0.1328          NaN          NaN          NaN  ...     NaN  \\\n",
      "1           0.1493          NaN          NaN          NaN  ...     NaN   \n",
      "2           0.1351          NaN          NaN          NaN  ...     NaN   \n",
      "3              NaN          NaN          NaN          NaN  ...   0.100   \n",
      "4              NaN          NaN          NaN          NaN  ...   0.550   \n",
      "...            ...          ...          ...          ...  ...     ...   \n",
      "13853          NaN          NaN          NaN          NaN  ...   1.330   \n",
      "13854          NaN          NaN          NaN          NaN  ...   2.180   \n",
      "13855          NaN       17.935       15.513       37.659  ...   0.355   \n",
      "13856          NaN       17.919       15.487       37.590  ...   0.329   \n",
      "13857          NaN       17.992       15.505       37.675  ...   0.533   \n",
      "\n",
      "       tb_ppm    th_ppm  tm_ppm  v_ppm   u_ppm  y_ppm  yb_ppm  zn_ppm  zr_ppm  \n",
      "0         NaN       NaN     NaN  174.0     NaN   14.0     NaN    66.0    56.0  \n",
      "1         NaN       NaN     NaN  236.0     NaN   19.0     NaN    61.0    90.0  \n",
      "2         NaN  6.000000     NaN  184.0     NaN   16.0     NaN    62.0    49.0  \n",
      "3       0.600  0.800000   0.400  315.7  0.3000   22.2    2.40     NaN    50.7  \n",
      "4       0.420  4.290000   0.190  142.6  0.9300   13.7    1.14    78.3   111.0  \n",
      "...       ...       ...     ...    ...     ...    ...     ...     ...     ...  \n",
      "13853   0.380  1.379000     NaN    NaN  0.4830   17.0    1.51     NaN   109.0  \n",
      "13854   0.760  3.012333     NaN    NaN  0.8925   19.0    2.32     NaN   165.0  \n",
      "13855   0.805  0.448000   0.444    NaN  0.1520   27.6    2.90    76.0   128.0  \n",
      "13856   0.858  0.381000   0.481    NaN  0.1320   29.7    3.13    82.0   123.0  \n",
      "13857   0.960  0.610000   0.517    NaN  0.2200   33.0    3.37    83.2   163.0  \n",
      "\n",
      "[13847 rows x 55 columns]\n",
      "13847\n"
     ]
    }
   ],
   "source": [
    "#Choosing  only the data that has location\n",
    "final_data.dropna(subset=['latitude', 'longitude'], inplace=True)\n",
    "print(final_data)\n",
    "print(len(final_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       labelSrNd  latitude  longitude  rb87_sr86  sr87_sr86  nd143_nd144   \n",
      "0              3  0.687148   0.861957  -0.000867   0.081755    -0.774560  \\\n",
      "1              3  0.687151   0.861957  -0.004441   0.077470    -0.427189   \n",
      "2              3  0.687156   0.861949  -0.005243   0.054618    -0.320056   \n",
      "3              3  0.705489   0.884736   0.000000  -0.108060     0.410398   \n",
      "4              3  0.739313   0.893431   0.000000   0.091324    -0.540815   \n",
      "...          ...       ...        ...        ...        ...          ...   \n",
      "15916          1  0.000000   0.000000   0.000000  -0.206324     0.916846   \n",
      "15917          1  0.000000   0.000000   0.000000  -0.203325     0.816205   \n",
      "15918          1  0.000000   0.000000   0.000000  -0.212465     0.881135   \n",
      "15919          1  0.000000   0.000000   0.000000  -0.204610     0.796727   \n",
      "15920          1  0.000000   0.000000   0.000000  -0.204039     0.900613   \n",
      "\n",
      "       sm147_nd144  pb206_pb204  pb207_pb204  pb208_pb204  ...    ta_ppm   \n",
      "0        -0.261708     0.000000     0.000000     0.000000  ...  0.000000  \\\n",
      "1         0.032258     0.000000     0.000000     0.000000  ...  0.000000   \n",
      "2        -0.220730     0.000000     0.000000     0.000000  ...  0.000000   \n",
      "3         0.000000     0.000000     0.000000     0.000000  ... -0.516161   \n",
      "4         0.000000     0.000000     0.000000     0.000000  ... -0.379551   \n",
      "...            ...          ...          ...          ...  ...       ...   \n",
      "15916     0.000000    -0.064399    -0.066419    -0.138644  ...  0.230642   \n",
      "15917     0.000000    -0.063284    -0.065308    -0.133729  ...  0.209391   \n",
      "15918     0.000000    -0.066631    -0.069011    -0.143938  ...  0.622258   \n",
      "15919     0.000000    -0.064081    -0.067530    -0.138392  ...  0.631365   \n",
      "15920     0.000000    -0.066710    -0.069752    -0.144190  ...  0.640473   \n",
      "\n",
      "         tb_ppm    th_ppm    tm_ppm     v_ppm     u_ppm     y_ppm    yb_ppm   \n",
      "0      0.000000  0.000000  0.000000 -0.682202  0.000000 -0.628216  0.000000  \\\n",
      "1      0.000000  0.000000  0.000000 -0.170827  0.000000 -0.388951  0.000000   \n",
      "2      0.000000  0.020840  0.000000 -0.599722  0.000000 -0.532510  0.000000   \n",
      "3     -0.222973 -0.136707  0.059748  0.486536 -0.168856 -0.235821  0.102990   \n",
      "4     -0.324744 -0.030969 -0.364615 -0.941188 -0.059663 -0.642571 -0.671128   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "15916  0.000000 -0.081566  0.000000  0.000000  0.000000 -0.087477 -0.271782   \n",
      "15917  0.000000 -0.083081  0.000000  0.000000  0.000000 -0.101833 -0.271782   \n",
      "15918  0.000000 -0.043088  0.000000  0.000000  0.000000  0.051296 -0.185768   \n",
      "15919  0.000000 -0.032181  0.000000  0.000000  0.000000  0.175714  0.016977   \n",
      "15920  0.000000 -0.040664  0.000000  0.000000  0.000000  0.089579 -0.155049   \n",
      "\n",
      "         zn_ppm    zr_ppm  \n",
      "0     -0.141918 -0.689131  \n",
      "1     -0.160004 -0.478283  \n",
      "2     -0.156387 -0.732541  \n",
      "3      0.000000 -0.721998  \n",
      "4     -0.097429 -0.348054  \n",
      "...         ...       ...  \n",
      "15916  0.000000  0.203870  \n",
      "15917  0.000000  0.148057  \n",
      "15918  0.000000  0.631766  \n",
      "15919  0.000000  0.681377  \n",
      "15920  0.000000  0.668975  \n",
      "\n",
      "[15921 rows x 55 columns]\n",
      "['labelSrNd', 'latitude', 'longitude', 'rb87_sr86', 'sr87_sr86', 'nd143_nd144', 'sm147_nd144', 'pb206_pb204', 'pb207_pb204', 'pb208_pb204', 'sio2', 'tio2', 'al2o3', 'fe2o3', 'fe2o3_tot', 'mgo', 'cao', 'mno', 'k2o', 'na2o', 'p2o5', 'ba_ppm', 'ce_ppm', 'co_ppm', 'cr_ppm', 'cs_ppm', 'cu_ppm', 'dy_ppm', 'er_ppm', 'eu_ppm', 'ga_ppm', 'gd_ppm', 'hf_ppm', 'ho_ppm', 'la_ppm', 'lu_ppm', 'nd_ppm', 'ni_ppm', 'nb_ppm', 'pb_ppm', 'pr_ppm', 'rb_ppm', 'sc_ppm', 'sm_ppm', 'sr_ppm', 'ta_ppm', 'tb_ppm', 'th_ppm', 'tm_ppm', 'v_ppm', 'u_ppm', 'y_ppm', 'yb_ppm', 'zn_ppm', 'zr_ppm']\n"
     ]
    }
   ],
   "source": [
    "labels_SrNd = final_data['labelSrNd']\n",
    "lat = final_data['latitude']\n",
    "long = final_data['longitude']\n",
    "\n",
    "min_latitude = -90\n",
    "max_latitude = 90\n",
    "min_longitude = -180\n",
    "max_longitude = 180\n",
    "\n",
    "#Normalizing the latitude and longitude values using min-max scaling established before\n",
    "lat = (lat - min_latitude) / (max_latitude - min_latitude)\n",
    "long = (long - min_longitude) / (max_longitude - min_longitude)\n",
    "#Normalizing my other features (chemical composition) with a Gaussian distribution \n",
    "normalized_final_data = (final_data - final_data.mean()) / (final_data.std())\n",
    "\n",
    "#Concatenating all my data: labels and normalized features (locations and chemical composition)\n",
    "normalized_final_data['labelSrNd'] = labels_SrNd\n",
    "normalized_final_data['latitude'] = lat\n",
    "normalized_final_data['longitude'] = long\n",
    "normalized_final_data = normalized_final_data.fillna(0)\n",
    "print(normalized_final_data)\n",
    "\n",
    "features=list(normalized_final_data.columns)\n",
    "print (features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        3\n",
      "1        3\n",
      "2        3\n",
      "3        3\n",
      "4        3\n",
      "        ..\n",
      "15916    1\n",
      "15917    1\n",
      "15918    1\n",
      "15919    1\n",
      "15920    1\n",
      "Name: labelSrNd, Length: 15921, dtype: int64\n",
      "15921\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin 0.0 - 0.8: 543\n",
      "Bin 0.8 - 1.6: 2478\n",
      "Bin 1.6 - 2.4000000000000004: 2356\n",
      "Bin 2.4000000000000004 - 3.2: 5493\n",
      "Bin 3.2 - 4.0: 5051\n"
     ]
    }
   ],
   "source": [
    "#Histogram to know how many samples there are per class (mantle source end-member) after all the filtering done. \n",
    "label_data= normalized_final_data['labelSrNd']\n",
    "print(label_data)\n",
    "print(len(label_data))\n",
    "\n",
    "label_data.hist(bins=5)  \n",
    "plt.show()\n",
    "hist, bins = np.histogram(label_data, bins=5) \n",
    "\n",
    "for i in range(len(hist)):\n",
    "    print(f\"Bin {bins[i]} - {bins[i+1]}: {hist[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       labelSrNd  latitude  longitude  rb87_sr86  sr87_sr86  nd143_nd144   \n",
      "0              3  0.687148   0.861957  -0.000867   0.081755    -0.774560  \\\n",
      "1              3  0.687151   0.861957  -0.004441   0.077470    -0.427189   \n",
      "2              3  0.687156   0.861949  -0.005243   0.054618    -0.320056   \n",
      "3              3  0.705489   0.884736   0.000000  -0.108060     0.410398   \n",
      "4              3  0.739313   0.893431   0.000000   0.091324    -0.540815   \n",
      "...          ...       ...        ...        ...        ...          ...   \n",
      "15916          1  0.000000   0.000000   0.000000  -0.206324     0.916846   \n",
      "15917          1  0.000000   0.000000   0.000000  -0.203325     0.816205   \n",
      "15918          1  0.000000   0.000000   0.000000  -0.212465     0.881135   \n",
      "15919          1  0.000000   0.000000   0.000000  -0.204610     0.796727   \n",
      "15920          1  0.000000   0.000000   0.000000  -0.204039     0.900613   \n",
      "\n",
      "       sm147_nd144  pb206_pb204  pb207_pb204  pb208_pb204  ...    ta_ppm   \n",
      "0        -0.261708     0.000000     0.000000     0.000000  ...  0.000000  \\\n",
      "1         0.032258     0.000000     0.000000     0.000000  ...  0.000000   \n",
      "2        -0.220730     0.000000     0.000000     0.000000  ...  0.000000   \n",
      "3         0.000000     0.000000     0.000000     0.000000  ... -0.516161   \n",
      "4         0.000000     0.000000     0.000000     0.000000  ... -0.379551   \n",
      "...            ...          ...          ...          ...  ...       ...   \n",
      "15916     0.000000    -0.064399    -0.066419    -0.138644  ...  0.230642   \n",
      "15917     0.000000    -0.063284    -0.065308    -0.133729  ...  0.209391   \n",
      "15918     0.000000    -0.066631    -0.069011    -0.143938  ...  0.622258   \n",
      "15919     0.000000    -0.064081    -0.067530    -0.138392  ...  0.631365   \n",
      "15920     0.000000    -0.066710    -0.069752    -0.144190  ...  0.640473   \n",
      "\n",
      "         tb_ppm    th_ppm    tm_ppm     v_ppm     u_ppm     y_ppm    yb_ppm   \n",
      "0      0.000000  0.000000  0.000000 -0.682202  0.000000 -0.628216  0.000000  \\\n",
      "1      0.000000  0.000000  0.000000 -0.170827  0.000000 -0.388951  0.000000   \n",
      "2      0.000000  0.020840  0.000000 -0.599722  0.000000 -0.532510  0.000000   \n",
      "3     -0.222973 -0.136707  0.059748  0.486536 -0.168856 -0.235821  0.102990   \n",
      "4     -0.324744 -0.030969 -0.364615 -0.941188 -0.059663 -0.642571 -0.671128   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "15916  0.000000 -0.081566  0.000000  0.000000  0.000000 -0.087477 -0.271782   \n",
      "15917  0.000000 -0.083081  0.000000  0.000000  0.000000 -0.101833 -0.271782   \n",
      "15918  0.000000 -0.043088  0.000000  0.000000  0.000000  0.051296 -0.185768   \n",
      "15919  0.000000 -0.032181  0.000000  0.000000  0.000000  0.175714  0.016977   \n",
      "15920  0.000000 -0.040664  0.000000  0.000000  0.000000  0.089579 -0.155049   \n",
      "\n",
      "         zn_ppm    zr_ppm  \n",
      "0     -0.141918 -0.689131  \n",
      "1     -0.160004 -0.478283  \n",
      "2     -0.156387 -0.732541  \n",
      "3      0.000000 -0.721998  \n",
      "4     -0.097429 -0.348054  \n",
      "...         ...       ...  \n",
      "15916  0.000000  0.203870  \n",
      "15917  0.000000  0.148057  \n",
      "15918  0.000000  0.631766  \n",
      "15919  0.000000  0.681377  \n",
      "15920  0.000000  0.668975  \n",
      "\n",
      "[15921 rows x 55 columns]\n"
     ]
    }
   ],
   "source": [
    "#In this block it is being chose which scenario I want to test. For example, if True means that I do not input any feature specified in the columns=[\"....,....\"] \n",
    "#Scenario 1: False--> All features are inputs: all isotopes inclusing Sr and Nd which were used for labeling and Pb, Sm and Rb.\n",
    "#Scenario 2 and 3: True\n",
    "drop_first_col= False\n",
    "\n",
    "if drop_first_col:\n",
    "    #Scenario 2: Sr-Nd not considered as input features  \n",
    "    normalized_final_data= normalized_final_data.drop(columns=['sr87_sr86', 'nd143_nd144'])\n",
    "    #Scenario 3: any isotope consider as input, only major and minor elements. \n",
    "    #normalized_final_data= normalized_final_data.drop(columns=['rb87_sr86', 'sr87_sr86', 'nd143_nd144', 'sm147_nd144', 'pb206_pb204', 'pb207_pb204', 'pb208_pb204'])\n",
    "\n",
    "print(normalized_final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12736\n",
      "1592\n",
      "1593\n"
     ]
    }
   ],
   "source": [
    "#Dividing my data set for training (80%), validation (20%) and testing (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, no_train_data = train_test_split(normalized_final_data, test_size=0.2, random_state=4)\n",
    "#train_data.to_csv('../DataFP/train_data.csv',index=False)\n",
    "print(len(train_data))\n",
    "\n",
    "val_data, test_data = train_test_split(no_train_data, test_size=0.5, random_state=4)\n",
    "#val_data.to_csv('../DataFP/val_data.csv',index=False)\n",
    "print(len(val_data))\n",
    "\n",
    "#test_data.to_csv('../DataFP/test_data.csv',index=False)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling my data set\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data.values\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        x = sample[1:] #my features: chemical composition including the major and minor elements plus Sr and Nd isotopes if required\n",
    "        y = sample[0] #mantle source label\n",
    "        return x, y\n",
    "\n",
    "\n",
    "datatrain = MyDataset(train_data)\n",
    "train_loader = DataLoader(datatrain, batch_size=16, shuffle=True) #batch_size can be modified (and was in order to see if there is any impact on the results)\n",
    "dataval=MyDataset(val_data)\n",
    "val_loader= DataLoader(dataval, batch_size=32) \n",
    "datatest=MyDataset(test_data)\n",
    "test_loader=DataLoader(datatest, batch_size=32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n",
      "Model saved\n",
      "Model saved\n",
      "Model saved\n",
      "Model saved\n",
      "Model saved\n",
      "Model saved\n",
      "Model saved\n",
      "Model saved\n",
      "Model saved\n",
      "Epoch 10/10000000: Train Loss: 1.1192, Train Acc: 0.7886, Train f1: 0.7181, Val Loss: 1.1161, Val Acc: 0.7940, Val f1: 0.7267\n",
      "Model saved\n",
      "Model saved\n",
      "Epoch 20/10000000: Train Loss: 1.1098, Train Acc: 0.7966, Train f1: 0.7264, Val Loss: 1.1201, Val Acc: 0.7839, Val f1: 0.7184\n",
      "Epoch 30/10000000: Train Loss: 1.1086, Train Acc: 0.7958, Train f1: 0.7256, Val Loss: 1.1104, Val Acc: 0.7946, Val f1: 0.7254\n",
      "Epoch 40/10000000: Train Loss: 1.1054, Train Acc: 0.8003, Train f1: 0.7308, Val Loss: 1.1143, Val Acc: 0.7902, Val f1: 0.7231\n",
      "Epoch 50/10000000: Train Loss: 1.1043, Train Acc: 0.8005, Train f1: 0.7334, Val Loss: 1.1165, Val Acc: 0.7871, Val f1: 0.7210\n",
      "Model saved\n",
      "Epoch 60/10000000: Train Loss: 1.1055, Train Acc: 0.7997, Train f1: 0.7326, Val Loss: 1.1181, Val Acc: 0.7852, Val f1: 0.7211\n",
      "Epoch 70/10000000: Train Loss: 1.1028, Train Acc: 0.8022, Train f1: 0.7358, Val Loss: 1.1180, Val Acc: 0.7864, Val f1: 0.7198\n",
      "Epoch 80/10000000: Train Loss: 1.1031, Train Acc: 0.8014, Train f1: 0.7346, Val Loss: 1.1112, Val Acc: 0.7940, Val f1: 0.7283\n",
      "Epoch 90/10000000: Train Loss: 1.1028, Train Acc: 0.8015, Train f1: 0.7352, Val Loss: 1.1148, Val Acc: 0.7896, Val f1: 0.7221\n",
      "Model saved\n",
      "Epoch 100/10000000: Train Loss: 1.1014, Train Acc: 0.8033, Train f1: 0.7364, Val Loss: 1.1061, Val Acc: 0.7977, Val f1: 0.7322\n",
      "Epoch 110/10000000: Train Loss: 1.1016, Train Acc: 0.8032, Train f1: 0.7369, Val Loss: 1.1211, Val Acc: 0.7833, Val f1: 0.7201\n",
      "Epoch 120/10000000: Train Loss: 1.1030, Train Acc: 0.8018, Train f1: 0.7355, Val Loss: 1.1019, Val Acc: 0.8028, Val f1: 0.7382\n",
      "Epoch 130/10000000: Train Loss: 1.1009, Train Acc: 0.8042, Train f1: 0.7383, Val Loss: 1.1124, Val Acc: 0.7927, Val f1: 0.7289\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[130], line 74\u001b[0m\n\u001b[0;32m     71\u001b[0m train_loss\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mloss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     73\u001b[0m \u001b[39m#Convert output probabilities to predicted class labels\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmax(outputs\u001b[39m.\u001b[39;49mdata, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     76\u001b[0m \u001b[39m#Accumulating the batch predictions and true labels to lists per each \u001b[39;00m\n\u001b[0;32m     77\u001b[0m batch_predictions\u001b[39m.\u001b[39mextend(predicted\u001b[39m.\u001b[39mtolist())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Defining my MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=0.0)  #Add dropout layer with dropout probability of 0.1\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc6 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Initialize the weights\n",
    "        #init.xavier_uniform_(self.fc1.weight)\n",
    "        #init.xavier_uniform_(self.fc2.weight)\n",
    "        #init.xavier_uniform_(self.fc3.weight)\n",
    "        #init.xavier_uniform_(self.fc4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout layer\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)  # Apply dropout layer\n",
    "        #Here I can \"play\" with the numer of hidden layers.\n",
    "        #x = torch.relu(self.fc3(x))\n",
    "        #x = torch.relu(self.fc4(x))\n",
    "        #x = self.dropout(x)\n",
    "        #x = torch.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        x=torch.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "#Hyperparameters\n",
    "input_size = 54  #Number of input features depending on which scenario I am working on: Major and minor elements plus isotopes (including Sr-Nd?). This value depends on True or False in block 5.\n",
    "hidden_size = 30 #Should be between input and output values. \n",
    "output_size = 5 #Mantle source types: DM, HIMU, EMI, BSE, PREMA\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10000000 #It will stop anyway due to the stopping criteria (Early stopping) I am adding below. \n",
    "\n",
    "# Starting the model\n",
    "model = MLP(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "#Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate,weight_decay=0*1e-5) #weight decay is my regularizer\n",
    "\n",
    "best_model = '../Results/best_model.csv' #Saving the model with the best accuracy in the validation to be used in the testing\n",
    "tolerance = 50 #Treshold to stop the model if it does not get better (For the Early stopping)\n",
    "aux_tolerance = tolerance\n",
    "best_acc = 0\n",
    "#Starting the loop for training and validation\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    batch_predictions = []\n",
    "    batch_true_labels = []\n",
    "    train_loss=0.0\n",
    "    \n",
    "    #Training loop\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device) #I am using the GPU\n",
    "        labels = labels.to(device)\n",
    "        #Forward pass\n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels.long())\n",
    "\n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss+=loss.item()\n",
    "\n",
    "        #Convert output probabilities to predicted class labels\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        #Accumulating the batch predictions and true labels to lists per each \n",
    "        batch_predictions.extend(predicted.tolist())\n",
    "        batch_true_labels.extend(labels.tolist())\n",
    "   \n",
    "    #Calculating metrics for each epoch\n",
    "    train_acc= accuracy_score(batch_true_labels, batch_predictions)  #Calculating the accuracy of all my batches from the accumulation of the batches predictions and true labels\n",
    "    train_f1= f1_score(batch_true_labels, batch_predictions, average='weighted')\n",
    "    train_loss/=i+1\n",
    "\n",
    "    #Validation loop\n",
    "    batch_val_predictions = []\n",
    "    batch_val_true_labels = []\n",
    "    val_loss=0.0\n",
    "    \n",
    "    for b, (inputs, labels) in enumerate(val_loader):\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs.float())\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            val_loss+=loss.item()\n",
    "            \n",
    "            # Convert output probabilities to predicted class labels\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Accumulating the batch predictions and true labels to lists per each \n",
    "            batch_val_predictions.extend(predicted.tolist())\n",
    "            batch_val_true_labels.extend(labels.tolist())\n",
    "    \n",
    "    # Calculate metrics for each epoch\n",
    "    val_acc= accuracy_score(batch_val_true_labels, batch_val_predictions)  #calculating the accuracy of all my batches from the accumulation of the batches predictions and true labels\n",
    "    val_f1 = f1_score(batch_val_true_labels, batch_val_predictions, average='weighted')\n",
    "    val_loss/=b+1\n",
    "\n",
    "    #Early stopping\n",
    "    #Comparing the results among my model to save the best one\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(),best_model )\n",
    "        print(\"Model saved\")\n",
    "        aux_tolerance = tolerance #Re-starting the process as the \"new\" calculated accuracy is better than one found before. So, the tolerance goes again to the initial treshold\n",
    "    else:\n",
    "        aux_tolerance -= 1 #If my model continuously is performing not good enough, meaning the accuracy is not changing or it is small, then I am decreasing the tolerance to stop soon as it is not performing well anyway\n",
    "\n",
    "    #Early Stopping itself\n",
    "    if aux_tolerance == 0:\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "    # Compute and print training loss every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        to_print = f'Epoch {epoch+1}/{num_epochs}: '\n",
    "        to_print += f'Train Loss: {train_loss:.4f}, '\n",
    "        to_print+=f'Train Acc: {train_acc:.4f}, '\n",
    "        to_print+=f'Train f1: {train_f1:.4f}, '\n",
    "        to_print+= f'Val Loss: {val_loss:.4f}, ' \n",
    "        to_print+=f'Val Acc: {val_acc:.4f}, '\n",
    "        to_print+=f'Val f1: {val_f1:.4f}'\n",
    "        print(to_print)\n",
    "\n",
    "print('Training finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7909604519774012\n",
      "[[  6   2  27   1  18]\n",
      " [  0   7   0   3 244]\n",
      " [  0   0 249   5   0]\n",
      " [  2   0   8 539   9]\n",
      " [  0   1   0  13 459]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.11      0.19        54\n",
      "         1.0       0.70      0.03      0.05       254\n",
      "         2.0       0.88      0.98      0.93       254\n",
      "         3.0       0.96      0.97      0.96       558\n",
      "         4.0       0.63      0.97      0.76       473\n",
      "\n",
      "    accuracy                           0.79      1593\n",
      "   macro avg       0.78      0.61      0.58      1593\n",
      "weighted avg       0.80      0.79      0.73      1593\n",
      "\n",
      "       DM_pred  HIMU_pred  EMI_pred  BSE_pred  PREMA_pred\n",
      "DM           6          2        27         1          18\n",
      "HIMU         0          7         0         3         244\n",
      "EMI          0          0       249         5           0\n",
      "BSE          2          0         8       539           9\n",
      "PREMA        0          1         0        13         459\n"
     ]
    }
   ],
   "source": [
    "#Testing loop\n",
    "\n",
    "test_predictions = []\n",
    "test_true_labels = []\n",
    "test_loss=0.0\n",
    "\n",
    "model.load_state_dict(torch.load(best_model))\n",
    "\n",
    "for c, (inputs, labels) in enumerate(test_loader):\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        test_loss+=loss.item()\n",
    "        \n",
    "        # Convert output probabilities to predicted class labels\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Accumulating the batch predictions and true labels to lists per each \n",
    "        test_predictions.extend(predicted.tolist())\n",
    "        test_true_labels.extend(labels.tolist())\n",
    "\n",
    "test_acc= accuracy_score(test_true_labels,test_predictions)  \n",
    "print(test_acc)\n",
    "cm = confusion_matrix(test_true_labels, test_predictions)\n",
    "report = classification_report(test_true_labels, test_predictions)\n",
    "print(cm)\n",
    "print(report)\n",
    "cm_df = pd.DataFrame(cm, index=['DM', 'HIMU','EMI','BSE','PREMA'], columns=['DM_pred', 'HIMU_pred','EMI_pred','BSE_pred','PREMA_pred'])\n",
    "cm_df.to_csv('../Results/confusion_matrix_test.csv')\n",
    "with open('../Results/classification_report_test.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "print(cm_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kernel Display Name",
   "language": "python",
   "name": "workstation_oficina"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
