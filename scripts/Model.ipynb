{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       labelSrNd   latitude   longitude  rb87_sr86  sr87_sr86  nd143_nd144   \n",
      "0              2  33.686551  130.304669     0.6262   0.705200     0.512554  \\\n",
      "1              2  33.687129  130.304404     0.5366   0.705170     0.512661   \n",
      "2              2  33.688168  130.301608     0.5165   0.705010     0.512694   \n",
      "3              2  36.988056  138.505000        NaN   0.703871     0.512919   \n",
      "4              2  43.076423  141.635239        NaN   0.705267     0.512626   \n",
      "...          ...        ...         ...        ...        ...          ...   \n",
      "15916          1        NaN         NaN        NaN   0.703183     0.513075   \n",
      "15917          1        NaN         NaN        NaN   0.703204     0.513044   \n",
      "15918          1        NaN         NaN        NaN   0.703140     0.513064   \n",
      "15919          1        NaN         NaN        NaN   0.703195     0.513038   \n",
      "15920          1        NaN         NaN        NaN   0.703199     0.513070   \n",
      "\n",
      "       sm147_nd144  pb206_pb204  pb207_pb204  pb208_pb204  ...  ta_ppm   \n",
      "0           0.1328          NaN          NaN          NaN  ...     NaN  \\\n",
      "1           0.1493          NaN          NaN          NaN  ...     NaN   \n",
      "2           0.1351          NaN          NaN          NaN  ...     NaN   \n",
      "3              NaN          NaN          NaN          NaN  ...    0.10   \n",
      "4              NaN          NaN          NaN          NaN  ...    0.55   \n",
      "...            ...          ...          ...          ...  ...     ...   \n",
      "15916          NaN       18.199       15.447       37.754  ...    2.56   \n",
      "15917          NaN       18.213       15.450       37.793  ...    2.49   \n",
      "15918          NaN       18.171       15.440       37.712  ...    3.85   \n",
      "15919          NaN       18.203       15.444       37.756  ...    3.88   \n",
      "15920          NaN       18.170       15.438       37.710  ...    3.91   \n",
      "\n",
      "       tb_ppm  th_ppm  tm_ppm  v_ppm  u_ppm  y_ppm  yb_ppm  zn_ppm  zr_ppm  \n",
      "0         NaN     NaN     NaN  174.0    NaN   14.0     NaN    66.0    56.0  \n",
      "1         NaN     NaN     NaN  236.0    NaN   19.0     NaN    61.0    90.0  \n",
      "2         NaN    6.00     NaN  184.0    NaN   16.0     NaN    62.0    49.0  \n",
      "3        0.60    0.80    0.40  315.7   0.30   22.2    2.40     NaN    50.7  \n",
      "4        0.42    4.29    0.19  142.6   0.93   13.7    1.14    78.3   111.0  \n",
      "...       ...     ...     ...    ...    ...    ...     ...     ...     ...  \n",
      "15916     NaN    2.62     NaN    NaN    NaN   25.3    1.79     NaN   200.0  \n",
      "15917     NaN    2.57     NaN    NaN    NaN   25.0    1.79     NaN   191.0  \n",
      "15918     NaN    3.89     NaN    NaN    NaN   28.2    1.93     NaN   269.0  \n",
      "15919     NaN    4.25     NaN    NaN    NaN   30.8    2.26     NaN   277.0  \n",
      "15920     NaN    3.97     NaN    NaN    NaN   29.0    1.98     NaN   275.0  \n",
      "\n",
      "[15921 rows x 56 columns]\n"
     ]
    }
   ],
   "source": [
    "#Data with Sr-Nd and Major, minor and trace elements. \n",
    "#Training my model with data that was labeled based on 143Nd/144Nd versus 87Sr/86Sr values. My data has 143Nd/144Nd versus 87Sr/86Sr values but also some major, minor and trace elements. \n",
    "\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "final_data=pd.read_csv('../DataFP/final_data.csv')\n",
    "print(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_SrNd = final_data['labelSrNd']\n",
    "lat = final_data['latitude']\n",
    "long = final_data['longitude']\n",
    "\n",
    "min_latitude = -90\n",
    "max_latitude = 90\n",
    "min_longitude = -180\n",
    "max_longitude = 180\n",
    "\n",
    "# Normalize latitude and longitude values using min-max scaling\n",
    "lat = (lat - min_latitude) / (max_latitude - min_latitude)\n",
    "long = (long - min_longitude) / (max_longitude - min_longitude)\n",
    "\n",
    "#Normalizing my data (chemical composition) with a Gaussian distribution \n",
    "normalized_final_data = (final_data - final_data.mean()) / (final_data.std())\n",
    "\n",
    "normalized_final_data['labelSrNd'] = labels_SrNd\n",
    "normalized_final_data['latitude'] = lat\n",
    "normalized_final_data['longitude'] = long\n",
    "normalized_final_data = normalized_final_data.fillna(0)\n",
    "\n",
    "#print(normalized_final_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       labelSrNd  latitude  longitude  rb87_sr86  sm147_nd144  pb206_pb204   \n",
      "0              2  0.687148   0.861957  -0.000867    -0.261708     0.000000  \\\n",
      "1              2  0.687151   0.861957  -0.004441     0.032258     0.000000   \n",
      "2              2  0.687156   0.861949  -0.005243    -0.220730     0.000000   \n",
      "3              2  0.705489   0.884736   0.000000     0.000000     0.000000   \n",
      "4              2  0.739313   0.893431   0.000000     0.000000     0.000000   \n",
      "...          ...       ...        ...        ...          ...          ...   \n",
      "15916          1  0.000000   0.000000   0.000000     0.000000    -0.064399   \n",
      "15917          1  0.000000   0.000000   0.000000     0.000000    -0.063284   \n",
      "15918          1  0.000000   0.000000   0.000000     0.000000    -0.066631   \n",
      "15919          1  0.000000   0.000000   0.000000     0.000000    -0.064081   \n",
      "15920          1  0.000000   0.000000   0.000000     0.000000    -0.066710   \n",
      "\n",
      "       pb207_pb204  pb208_pb204      sio2      tio2  ...    ta_ppm    tb_ppm   \n",
      "0         0.000000     0.000000  0.430641 -1.192988  ...  0.000000  0.000000  \\\n",
      "1         0.000000     0.000000  0.828414 -1.070478  ...  0.000000  0.000000   \n",
      "2         0.000000     0.000000  0.833563 -1.157985  ...  0.000000  0.000000   \n",
      "3         0.000000     0.000000  0.560658 -0.877961  ... -0.516161 -0.222973   \n",
      "4         0.000000     0.000000  0.335382 -0.851709  ... -0.379551 -0.324744   \n",
      "...            ...          ...       ...       ...  ...       ...       ...   \n",
      "15916    -0.066419    -0.138644 -0.414035  1.295478  ...  0.230642  0.000000   \n",
      "15917    -0.065308    -0.133729 -0.524625  1.301121  ...  0.209391  0.000000   \n",
      "15918    -0.069011    -0.143938 -0.563374  1.779921  ...  0.622258  0.000000   \n",
      "15919    -0.067530    -0.138392 -0.209712  1.113470  ...  0.631365  0.000000   \n",
      "15920    -0.069752    -0.144190 -0.538220  1.734698  ...  0.640473  0.000000   \n",
      "\n",
      "         th_ppm    tm_ppm     v_ppm     u_ppm     y_ppm    yb_ppm    zn_ppm   \n",
      "0      0.000000  0.000000 -0.682202  0.000000 -0.628216  0.000000 -0.141918  \\\n",
      "1      0.000000  0.000000 -0.170827  0.000000 -0.388951  0.000000 -0.160004   \n",
      "2      0.020840  0.000000 -0.599722  0.000000 -0.532510  0.000000 -0.156387   \n",
      "3     -0.136707  0.059748  0.486536 -0.168856 -0.235821  0.102990  0.000000   \n",
      "4     -0.030969 -0.364615 -0.941188 -0.059663 -0.642571 -0.671128 -0.097429   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "15916 -0.081566  0.000000  0.000000  0.000000 -0.087477 -0.271782  0.000000   \n",
      "15917 -0.083081  0.000000  0.000000  0.000000 -0.101833 -0.271782  0.000000   \n",
      "15918 -0.043088  0.000000  0.000000  0.000000  0.051296 -0.185768  0.000000   \n",
      "15919 -0.032181  0.000000  0.000000  0.000000  0.175714  0.016977  0.000000   \n",
      "15920 -0.040664  0.000000  0.000000  0.000000  0.089579 -0.155049  0.000000   \n",
      "\n",
      "         zr_ppm  \n",
      "0     -0.689131  \n",
      "1     -0.478283  \n",
      "2     -0.732541  \n",
      "3     -0.721998  \n",
      "4     -0.348054  \n",
      "...         ...  \n",
      "15916  0.203870  \n",
      "15917  0.148057  \n",
      "15918  0.631766  \n",
      "15919  0.681377  \n",
      "15920  0.668975  \n",
      "\n",
      "[15921 rows x 54 columns]\n"
     ]
    }
   ],
   "source": [
    "drop_first_col=True #True for only major, minor and trace. #False when including the isotopes Sr and Nd that I used for labeling the data\n",
    "if drop_first_col:\n",
    "    normalized_final_data= normalized_final_data.drop(columns=['sr87_sr86','nd143_nd144'])\n",
    "\n",
    "\n",
    "print(normalized_final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12736\n",
      "1592\n",
      "1593\n"
     ]
    }
   ],
   "source": [
    "#Dividing my data set for training (80%), validation (20%) and test (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, no_train_data = train_test_split(normalized_final_data, test_size=0.2, random_state=42)\n",
    "#train_data.to_csv('../DataFP/train_data.csv',index=False)\n",
    "print(len(train_data))\n",
    "\n",
    "val_data, test_data = train_test_split(no_train_data, test_size=0.5, random_state=42)\n",
    "#val_data.to_csv('../DataFP/val_data.csv',index=False)\n",
    "print(len(val_data))\n",
    "\n",
    "#test_data.to_csv('../DataFP/test_data.csv',index=False)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data.values\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        #print(type(self.data))\n",
    "        sample = self.data[idx]\n",
    "        x = sample[1:] #all my features- chemical composition: Major and minor elements plus Sr and Nd isotopes\n",
    "        y = sample[0] #mantle source label\n",
    "        return x, y\n",
    "\n",
    "\n",
    "datatrain = MyDataset(train_data)\n",
    "train_loader = DataLoader(datatrain, batch_size=32, shuffle=True)\n",
    "dataval=MyDataset(val_data)\n",
    "val_loader= DataLoader(dataval, batch_size=32)\n",
    "datatest=MyDataset(test_data)\n",
    "test_loader=DataLoader(datatest, batch_size=32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/500: Train Loss: 1.2829, Train Acc: 0.6194, Train f1: 0.5454, Val Loss: 1.2676, Val Acc: 0.6382, Val f1: 0.5704\n",
      "Epoch 20/500: Train Loss: 1.2565, Train Acc: 0.6483, Train f1: 0.5806, Val Loss: 1.2549, Val Acc: 0.6501, Val f1: 0.5831\n",
      "Epoch 30/500: Train Loss: 1.2435, Train Acc: 0.6594, Train f1: 0.5931, Val Loss: 1.2500, Val Acc: 0.6501, Val f1: 0.5858\n",
      "Epoch 40/500: Train Loss: 1.2362, Train Acc: 0.6672, Train f1: 0.6021, Val Loss: 1.2490, Val Acc: 0.6482, Val f1: 0.5864\n",
      "Epoch 50/500: Train Loss: 1.2297, Train Acc: 0.6736, Train f1: 0.6100, Val Loss: 1.2449, Val Acc: 0.6533, Val f1: 0.5912\n",
      "Epoch 60/500: Train Loss: 1.2224, Train Acc: 0.6810, Train f1: 0.6171, Val Loss: 1.2354, Val Acc: 0.6633, Val f1: 0.6031\n",
      "Epoch 70/500: Train Loss: 1.2201, Train Acc: 0.6838, Train f1: 0.6221, Val Loss: 1.2374, Val Acc: 0.6633, Val f1: 0.6019\n",
      "Epoch 80/500: Train Loss: 1.2122, Train Acc: 0.6918, Train f1: 0.6292, Val Loss: 1.2404, Val Acc: 0.6570, Val f1: 0.5934\n",
      "Epoch 90/500: Train Loss: 1.2124, Train Acc: 0.6917, Train f1: 0.6290, Val Loss: 1.2401, Val Acc: 0.6608, Val f1: 0.6077\n",
      "Epoch 100/500: Train Loss: 1.2067, Train Acc: 0.6958, Train f1: 0.6329, Val Loss: 1.2328, Val Acc: 0.6715, Val f1: 0.6090\n",
      "Epoch 110/500: Train Loss: 1.2065, Train Acc: 0.6963, Train f1: 0.6342, Val Loss: 1.2282, Val Acc: 0.6721, Val f1: 0.6124\n",
      "Epoch 120/500: Train Loss: 1.2031, Train Acc: 0.7006, Train f1: 0.6380, Val Loss: 1.2401, Val Acc: 0.6608, Val f1: 0.6076\n",
      "Epoch 130/500: Train Loss: 1.2019, Train Acc: 0.7022, Train f1: 0.6397, Val Loss: 1.2298, Val Acc: 0.6746, Val f1: 0.6145\n",
      "Epoch 140/500: Train Loss: 1.1989, Train Acc: 0.7062, Train f1: 0.6437, Val Loss: 1.2268, Val Acc: 0.6740, Val f1: 0.6138\n",
      "Epoch 150/500: Train Loss: 1.2003, Train Acc: 0.7026, Train f1: 0.6397, Val Loss: 1.2266, Val Acc: 0.6778, Val f1: 0.6179\n",
      "Epoch 160/500: Train Loss: 1.1966, Train Acc: 0.7075, Train f1: 0.6450, Val Loss: 1.2230, Val Acc: 0.6809, Val f1: 0.6231\n",
      "Epoch 170/500: Train Loss: 1.1950, Train Acc: 0.7092, Train f1: 0.6469, Val Loss: 1.2236, Val Acc: 0.6765, Val f1: 0.6204\n",
      "Epoch 180/500: Train Loss: 1.1962, Train Acc: 0.7078, Train f1: 0.6453, Val Loss: 1.2200, Val Acc: 0.6803, Val f1: 0.6187\n",
      "Epoch 190/500: Train Loss: 1.1933, Train Acc: 0.7098, Train f1: 0.6483, Val Loss: 1.2205, Val Acc: 0.6828, Val f1: 0.6225\n",
      "Epoch 200/500: Train Loss: 1.1920, Train Acc: 0.7118, Train f1: 0.6487, Val Loss: 1.2316, Val Acc: 0.6677, Val f1: 0.6105\n",
      "Epoch 210/500: Train Loss: 1.1909, Train Acc: 0.7129, Train f1: 0.6505, Val Loss: 1.2259, Val Acc: 0.6759, Val f1: 0.6156\n",
      "Epoch 220/500: Train Loss: 1.1898, Train Acc: 0.7149, Train f1: 0.6531, Val Loss: 1.2325, Val Acc: 0.6696, Val f1: 0.6111\n",
      "Epoch 230/500: Train Loss: 1.1893, Train Acc: 0.7151, Train f1: 0.6533, Val Loss: 1.2280, Val Acc: 0.6778, Val f1: 0.6181\n",
      "Epoch 240/500: Train Loss: 1.1888, Train Acc: 0.7157, Train f1: 0.6540, Val Loss: 1.2278, Val Acc: 0.6753, Val f1: 0.6132\n",
      "Epoch 250/500: Train Loss: 1.1876, Train Acc: 0.7169, Train f1: 0.6544, Val Loss: 1.2231, Val Acc: 0.6784, Val f1: 0.6181\n",
      "Epoch 260/500: Train Loss: 1.1885, Train Acc: 0.7157, Train f1: 0.6535, Val Loss: 1.2327, Val Acc: 0.6677, Val f1: 0.6073\n",
      "Epoch 270/500: Train Loss: 1.1887, Train Acc: 0.7153, Train f1: 0.6534, Val Loss: 1.2208, Val Acc: 0.6815, Val f1: 0.6243\n",
      "Epoch 280/500: Train Loss: 1.1866, Train Acc: 0.7180, Train f1: 0.6561, Val Loss: 1.2263, Val Acc: 0.6759, Val f1: 0.6180\n",
      "Epoch 290/500: Train Loss: 1.1870, Train Acc: 0.7180, Train f1: 0.6557, Val Loss: 1.2253, Val Acc: 0.6759, Val f1: 0.6144\n",
      "Epoch 300/500: Train Loss: 1.1875, Train Acc: 0.7169, Train f1: 0.6549, Val Loss: 1.2194, Val Acc: 0.6828, Val f1: 0.6214\n",
      "Epoch 310/500: Train Loss: 1.1818, Train Acc: 0.7225, Train f1: 0.6602, Val Loss: 1.2177, Val Acc: 0.6866, Val f1: 0.6259\n",
      "Epoch 320/500: Train Loss: 1.1831, Train Acc: 0.7207, Train f1: 0.6584, Val Loss: 1.2296, Val Acc: 0.6715, Val f1: 0.6148\n",
      "Epoch 330/500: Train Loss: 1.1837, Train Acc: 0.7204, Train f1: 0.6584, Val Loss: 1.2294, Val Acc: 0.6715, Val f1: 0.6147\n",
      "Epoch 340/500: Train Loss: 1.1845, Train Acc: 0.7191, Train f1: 0.6566, Val Loss: 1.2149, Val Acc: 0.6884, Val f1: 0.6288\n",
      "Epoch 350/500: Train Loss: 1.1852, Train Acc: 0.7188, Train f1: 0.6568, Val Loss: 1.2204, Val Acc: 0.6815, Val f1: 0.6198\n",
      "Epoch 360/500: Train Loss: 1.1861, Train Acc: 0.7171, Train f1: 0.6550, Val Loss: 1.2202, Val Acc: 0.6822, Val f1: 0.6227\n",
      "Epoch 370/500: Train Loss: 1.1831, Train Acc: 0.7211, Train f1: 0.6593, Val Loss: 1.2246, Val Acc: 0.6771, Val f1: 0.6159\n",
      "Epoch 380/500: Train Loss: 1.1816, Train Acc: 0.7228, Train f1: 0.6603, Val Loss: 1.2259, Val Acc: 0.6740, Val f1: 0.6160\n",
      "Epoch 390/500: Train Loss: 1.1830, Train Acc: 0.7206, Train f1: 0.6587, Val Loss: 1.2265, Val Acc: 0.6753, Val f1: 0.6160\n",
      "Epoch 400/500: Train Loss: 1.1826, Train Acc: 0.7216, Train f1: 0.6593, Val Loss: 1.2326, Val Acc: 0.6696, Val f1: 0.6097\n",
      "Epoch 410/500: Train Loss: 1.1838, Train Acc: 0.7198, Train f1: 0.6578, Val Loss: 1.2202, Val Acc: 0.6840, Val f1: 0.6245\n",
      "Epoch 420/500: Train Loss: 1.1800, Train Acc: 0.7244, Train f1: 0.6623, Val Loss: 1.2203, Val Acc: 0.6834, Val f1: 0.6236\n",
      "Epoch 430/500: Train Loss: 1.1830, Train Acc: 0.7213, Train f1: 0.6588, Val Loss: 1.2228, Val Acc: 0.6778, Val f1: 0.6187\n",
      "Epoch 440/500: Train Loss: 1.1797, Train Acc: 0.7253, Train f1: 0.6628, Val Loss: 1.2177, Val Acc: 0.6853, Val f1: 0.6265\n",
      "Epoch 450/500: Train Loss: 1.1804, Train Acc: 0.7239, Train f1: 0.6615, Val Loss: 1.2214, Val Acc: 0.6847, Val f1: 0.6235\n",
      "Epoch 460/500: Train Loss: 1.1790, Train Acc: 0.7249, Train f1: 0.6631, Val Loss: 1.2332, Val Acc: 0.6671, Val f1: 0.6093\n",
      "Epoch 470/500: Train Loss: 1.1803, Train Acc: 0.7239, Train f1: 0.6617, Val Loss: 1.2292, Val Acc: 0.6746, Val f1: 0.6147\n",
      "Epoch 480/500: Train Loss: 1.1765, Train Acc: 0.7277, Train f1: 0.6654, Val Loss: 1.2304, Val Acc: 0.6715, Val f1: 0.6133\n",
      "Epoch 490/500: Train Loss: 1.1813, Train Acc: 0.7233, Train f1: 0.6617, Val Loss: 1.2214, Val Acc: 0.6809, Val f1: 0.6208\n",
      "Epoch 500/500: Train Loss: 1.1753, Train Acc: 0.7293, Train f1: 0.6669, Val Loss: 1.2143, Val Acc: 0.6903, Val f1: 0.6289\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "# Define your MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=0.1)  # Add dropout layer with dropout probability of 0.1\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "         # Initialize the weights\n",
    "        init.xavier_uniform_(self.fc1.weight)\n",
    "        init.xavier_uniform_(self.fc2.weight)\n",
    "        init.xavier_uniform_(self.fc3.weight)\n",
    "        init.xavier_uniform_(self.fc4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout layer\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)  # Apply dropout layer\n",
    "        #x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        x=torch.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "#Hyperparameters\n",
    "input_size = 53  #Number of features: Major and minor elements plus Sr and Nd isotopes\n",
    "hidden_size = 25 #I define it, should be between input and output\n",
    "output_size = 5 #mantle source types: DM, HIMU, EMI. BSE, PREMA\n",
    "learning_rate = 0.001\n",
    "num_epochs = 500\n",
    "\n",
    "# Instantiate your model\n",
    "model = MLP(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)#,weight_decay=1e-5)\n",
    "\n",
    "\n",
    "#Start the loop for training and validation\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    batch_predictions = []\n",
    "    batch_true_labels = []\n",
    "    train_loss=0.0\n",
    "    \n",
    "    # Training\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels.long())\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss+=loss.item()\n",
    "\n",
    "        # Convert output probabilities to predicted class labels\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Accumulating the batch predictions and true labels to lists per each \n",
    "        batch_predictions.extend(predicted.tolist())\n",
    "        batch_true_labels.extend(labels.tolist())\n",
    "   \n",
    "    # Calculate metrics for each epoch\n",
    "    train_acc= accuracy_score(batch_true_labels, batch_predictions)  #calculating the accuracy of all my batches from the accumulation of the batches predictions and true labels\n",
    "    train_f1= f1_score(batch_true_labels, batch_predictions, average='weighted')\n",
    "    train_loss/=i+1\n",
    "\n",
    "    #_____________________________________________________________________________________________________________________________\n",
    "    #Validation\n",
    "    batch_val_predictions = []\n",
    "    batch_val_true_labels = []\n",
    "    val_loss=0.0\n",
    "    \n",
    "    for b, (inputs, labels) in enumerate(val_loader):\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            outputs = model(inputs.float())\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            val_loss+=loss.item()\n",
    "            \n",
    "            # Convert output probabilities to predicted class labels\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Accumulating the batch predictions and true labels to lists per each \n",
    "            batch_val_predictions.extend(predicted.tolist())\n",
    "            batch_val_true_labels.extend(labels.tolist())\n",
    "    \n",
    "    # Calculate metrics for each epoch\n",
    "    val_acc= accuracy_score(batch_val_true_labels, batch_val_predictions)  #calculating the accuracy of all my batches from the accumulation of the batches predictions and true labels\n",
    "    val_f1 = f1_score(batch_val_true_labels, batch_val_predictions, average='weighted')\n",
    "    val_loss/=b+1\n",
    "\n",
    "    # Compute and print training loss every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        to_print = f'Epoch {epoch+1}/{num_epochs}: '\n",
    "        to_print += f'Train Loss: {train_loss:.4f}, '\n",
    "        to_print+=f'Train Acc: {train_acc:.4f}, '\n",
    "        to_print+=f'Train f1: {train_f1:.4f}, '\n",
    "        to_print+= f'Val Loss: {val_loss:.4f}, ' \n",
    "        to_print+=f'Val Acc: {val_acc:.4f}, '\n",
    "        to_print+=f'Val f1: {val_f1:.4f}'\n",
    "        print(to_print)\n",
    "\n",
    "print('Training finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6911487758945386\n",
      "[[  0  37  98   0  27]\n",
      " [  0 217  80   0  20]\n",
      " [  0  34 787   0  33]\n",
      " [  0   0  55   0   0]\n",
      " [  0  36  72   0  97]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       162\n",
      "         1.0       0.67      0.68      0.68       317\n",
      "         2.0       0.72      0.92      0.81       854\n",
      "         3.0       0.00      0.00      0.00        55\n",
      "         4.0       0.55      0.47      0.51       205\n",
      "\n",
      "    accuracy                           0.69      1593\n",
      "   macro avg       0.39      0.42      0.40      1593\n",
      "weighted avg       0.59      0.69      0.63      1593\n",
      "\n",
      "       DM_pred  HIMU_pred  EMI_pred  BSE_pred  PREMA_pred\n",
      "DM           0         37        98         0          27\n",
      "HIMU         0        217        80         0          20\n",
      "EMI          0         34       787         0          33\n",
      "BSE          0          0        55         0           0\n",
      "PREMA        0         36        72         0          97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garciaer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\garciaer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\garciaer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "test_predictions = []\n",
    "test_true_labels = []\n",
    "test_loss=0.0\n",
    "\n",
    "for c, (inputs, labels) in enumerate(test_loader):\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        test_loss+=loss.item()\n",
    "        \n",
    "        # Convert output probabilities to predicted class labels\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Accumulating the batch predictions and true labels to lists per each \n",
    "        test_predictions.extend(predicted.tolist())\n",
    "        test_true_labels.extend(labels.tolist())\n",
    "\n",
    "test_acc= accuracy_score(test_true_labels,test_predictions)  \n",
    "print(test_acc)\n",
    "cm = confusion_matrix(test_true_labels, test_predictions)\n",
    "report = classification_report(test_true_labels, test_predictions)\n",
    "print(cm)\n",
    "print(report)\n",
    "cm_df = pd.DataFrame(cm, index=['DM', 'HIMU','EMI','BSE','PREMA'], columns=['DM_pred', 'HIMU_pred','EMI_pred','BSE_pred','PREMA_pred'])\n",
    "cm_df.to_csv('../Results/confusion_matrix_500.csv')\n",
    "with open('../Results/classification_report_500.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "print(cm_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data with Sr-Nd and Major, minor and trace elements. \n",
    "#Training my model with data that was labeled based on 143Nd/144Nd versus 87Sr/86Sr values. My data has 143Nd/144Nd versus 87Sr/86Sr values but also some major, minor and trace elements. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
